---
title: "Understanding the Statistical Distributions"
author: "AbdulHafiz Abba"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## **Statistical Distributions**
In statistics, there are many different probability distributions that are used to model and describe various types of data and random phenomena. These distributions can be broadly categorized into two main types: discrete and continuous distributions. Here are some of the most commonly encountered probability distributions in each category:

## **Discrete Probability Distributions**:

### 1. Bernoulli Distribution:
Models a binary outcome (e.g., success/failure) with a single parameter p representing the probability of success.

Imagine you're playing a really simple game, like flipping a coin. In this game, there are only two possible outcomes: you can either win or lose. Let's call winning a "success" and losing a "failure."

Now, let's say you want to know the chances of winning (getting a success) in this game. The Bernoulli Distribution helps you figure that out.

It's like asking, "What's the probability that I'll win when I play this game once?" The Bernoulli Distribution gives you a way to calculate that probability.

So, if you're flipping a fair coin, you have a 50% chance of winning (getting a head) and a 50% chance of losing (getting a tail). In this case, the Bernoulli Distribution is pretty straightforward. It's like a simple tool for finding the probability of success in a basic "win or lose" situation.


$P(X = x) = p^x \cdot (1 - p)^{1-x}$

```{r}
# Set the seed for reproducibility
set.seed(123)

# Number of trials (sample size)
n <- 1000

# Probability of success (e.g., getting a "1" or "success")
p <- 0.3

# Generate random samples from a Bernoulli distribution
bernoulli_samples <- rbinom(n, size = 1, prob = p)

# Display a histogram to visualize the distribution
hist(bernoulli_samples, breaks = 2, main = "Bernoulli Distribution",
     xlab = "Outcome (0 or 1)", ylab = "Frequency", col = "lightblue")

# Add labels for the two outcomes
axis(1, at = c(0, 1), labels = c("Failure (0)", "Success (1)"))

```
To demonstrate the Central Limit Theorem (CLT) for a Bernoulli distribution in R, you can simulate a large number of Bernoulli trials and then calculate the sample mean for each trial. Here's an example code to do that:
```{r}
# Set the seed for reproducibility
set.seed(123)

# Parameters for the Bernoulli distribution
p <- 0.3  # Probability of success (e.g., getting a "1")

# Number of trials to simulate
num_trials <- 1000

# Number of Bernoulli trials in each sample
sample_size <- 100

# Initialize an empty vector to store sample means
sample_means <- numeric(num_trials)

# Simulate the CLT by repeating the sampling process many times
for (i in 1:num_trials) {
  # Simulate a sample of Bernoulli trials
  sample <- rbinom(sample_size, size = 1, prob = p)
  
  # Calculate the sample mean
  sample_means[i] <- mean(sample)
}

# Plot the sampling distribution of the sample mean
hist(sample_means, breaks = 30, main = "Sampling Distribution of Sample Mean (Bernoulli)",
     xlab = "Sample Mean", ylab = "Frequency", col = "lightblue")

# Add a vertical line at the population mean (expected value)
abline(v = p, col = "red", lwd = 2)

# Add a legend
legend("topright", legend = c("Population Mean", "Sample Mean"), col = c("red", "black"), lwd = 2)

```


### 2. Binomial Distribution: 

Represents the number of successes in a fixed number of independent Bernoulli trials.
**Binomial Distribution in Simple Terms:**

Imagine you're flipping a fair coin, and you want to know the probability of getting a certain number of heads in a fixed number of flips. That's where the binomial distribution comes in.

It's like asking, "What are the chances of getting exactly 3 heads if I flip this coin 10 times?" Or, "What's the probability of getting 7 heads in 20 flips?" The binomial distribution helps you answer these questions.

In simpler words, it's a math tool that helps you figure out the probabilities of getting a specific number of "successes" (like heads) in a set number of tries (like coin flips), assuming each try is independent and has the same probability of success.

$P(X = k) = \binom{n}{k} p^k (1-p)^{n-k}$


- \(P(X = k)\) represents the probability of getting exactly \(k\) successes.
- \(n\) is the total number of trials or attempts.
- \(k\) is the number of successes you want to find the probability for.
- \(p\) is the probability of success in a single trial.
- \(\binom{n}{k}\) represents the binomial coefficient, which calculates the number of ways to choose \(k\) successes out of \(n\) trials.

You can use this formula to calculate the probability of achieving a specific number of successes in a given number of trials with a known probability of success for each trial.

### 3. Poisson Distribution:
Describes the number of events occurring in a fixed interval of time or space, assuming a known average rate.

### 4. Geometric Distribution:
Models the number of trials needed to achieve the first success in a series of Bernoulli trials.

### 5. Hypergeometric Distribution:
Used for sampling without replacement, such as drawing items from a finite population without replacement.

### 6. Negative Binomial Distribution:
Represents the number of trials required for a given number of successes in a series of Bernoulli trials.

### 7. Multinomial Distribution:
Generalization of the binomial distribution for more than two categories.

## **Continuous Probability Distributions**:

### 9. Normal (Gaussian) Distribution:
Often referred to as the bell curve, it's used to model many natural phenomena due to its symmetry and central limit theorem properties.

### 10. Uniform Distribution:
All values within an interval are equally likely.

### 11. Exponential Distribution:
Describes the time between events in a Poisson process (continuous analog of the Poisson distribution).

### 12. Gamma Distribution:
Generalizes the exponential distribution and is often used in reliability analysis.


### 13. Weibull Distribution: 
Commonly used in survival analysis to model time-to-failure data.

### 14. Log-Normal Distribution:
Used to model data that follows a log-normal pattern after taking the natural logarithm.

### 15 .Beta Distribution: 
Used to model random variables constrained to the interval [0, 1], such as probabilities.

### 16. Cauchy Distribution: 
Known for its heavy tails and lack of finite moments, which makes it sensitive to outliers.

### 17. Chi-Square Distribution: 
Arises in hypothesis testing and is the distribution of the sum of squared standard normal random variables.

### 18. Student's t-Distribution: 
Used in hypothesis testing for small sample sizes when the population standard deviation is unknown.

### 19. F-Distribution: 
Often used in analysis of variance (ANOVA) and regression analysis.

These are just some of the most commonly encountered probability distributions in statistics. There are many other specialized distributions that are used in various fields of study and for modeling different types of data. The choice of distribution depends on the nature of the data and the statistical problem at hand.
